# Первая часть ДЗ на каникулы)
# 1. Спарсить с сайта https://spys.one инфу о прокси-серверах(табличку со скрина)
# сохранить ее в файл, где каждая строчка соответствует строке из таблицы в формате json.

# 2. Выбрать любую страничку в Википедии и сохранить в файл все ссылки с этой страницы построчно.
# Первая строка - ссылка на саму страницу,
# все последующие - ссылки на этой странице

# 3. Первую программу-парсер сделать в стиле ООП с классами,
# реализующими методы получения объекта BeautifulSoup
# и самой инфы о прокси)

# Все это залить на свой гитхаб. Мне прислать ссылки на pull request

import requests
from bs4 import BeautifulSoup
# from fake_useragent import useragent
# import fake_user_agent
import useragent
import re
import json

headers = {
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}
url = 'https://spys.one/'
# response = requests.get(url, headers={'User-agent': ua})
response = requests.get(url, headers=headers)
html = response.text
# получили код страницы

# Создаем объект класса BeautifulSoup. 1-й аргумент - код html-страницы, 2-й аргумент - парсер для html ('html.parser')
soup = BeautifulSoup(html, 'html.parser')

# получаем все динамические данные таблицы - font
# table_online_proxy = soup.find_all('font')
table_online_proxy1 = soup.findAll('td', colspan='1')
# print(table_online_proxy?)   # поставить корректный ввод

# или вот таким образом
# table_online_proxy1 = soup.findAll('td', colspan='1')
# print(table_online_proxy1)
# [<td colspan="1"><font class="spy2">IP адрес и порт</font></td>, <td colspan="1"><font class="spy2">Тип</font></td>,
# <td colspan="1"><font class="spy2">Анонимность</font></td>, <td colspan="1"><font class="spy2">Страна</font></td>,
# <td colspan="1"><font class="spy2">Аптайм</font></td>, <td colspan="1"><font class="spy2">Дата проверки</font></td>,
# <td colspan="1"><font class="spy14">41.33.254.186:1976</font></td>, <td colspan="1"><font class="spy1">
# <acronym title="Mikrotik">HTTP <font class="spy14.................

row_1 = []
for row in table_online_proxy1:
        row_1.append(row.text)
# print(row_1)
row_1_2 = row_1[6:-2]
row_2 = list(zip(row_1_2[::6],row_1_2[1::6],row_1_2[2::6],row_1_2[3::6],row_1_2[4::6],row_1_2[5::6]))
# вывод на экран
for validated_rows in row_2:
        print(json.dumps(validated_rows, ensure_ascii=False))
# записываем данные в фаил
with open(file='proxy.txt', mode='w', encoding='utf-8') as file:
        for validated_rows in row_2:
                file.write(f'{json.dumps(validated_rows,ensure_ascii=False)}\n')


# вывод
# ["89.249.246.130:8080", "HTTPS !", "NOA", "Россия ", "21% (43) -", "04:05:23 14:45:37"]
# ["178.32.196.197:11211", "HTTP", "ANM", "Франция ", "24% (8) -", "04:05:23 14:45:19"]
# ["152.231.17.229:999", "HTTP !", "NOA", "Аргентина ", "12% (11) -", "04:05:23 14:43:11"]
# ["203.137.122.39:5000", "HTTP", "ANM", "Япония ", "31% (13) -", "04:05:23 14:42:59"]
# ["190.208.40.190:9480", "HTTPS", "ANM", "Чили !", "80% (16) +", "04:05:23 14:41:58"]
# ["95.217.20.186:8080", "HTTPS !", "NOA", "Финляндия ", "100% (14) +", "04:05:23 14:41:10"]
# ["82.165.184.53:80", "HTTP", "HIA", "Германия ", "25% (674) -", "04:05:23 14:39:58"]
# ["23.88.33.250:80", "HTTP", "ANM", "Германия ", "97% (96) +", "04:05:23 14:38:40"]
# ["34.174.209.252:8585", "HTTP", "ANM", "США ", "50% (1) -", "04:05:23 14:37:48"]
# ["3.38.96.91:3128", "HTTP !", "NOA", "Южная Корея ", "11% (4) -", "04:05:23 14:36:05"]
# ["181.129.94.58:999", "HTTPS !", "NOA", "Колумбиа !", "18% (18) -", "04:05:23 14:35:08"]
# ["40.76.245.70:8080", "HTTPS", "HIA", "США ", "92% (11) +", "04:05:23 14:34:21"]
# ["202.138.240.24:9876", "HTTP !", "NOA", "Индонезия ", "12% (14) -", "04:05:23 14:33:43"]
# ["103.69.108.78:8191", "HTTPS", "HIA", "Филиппины ", "79% (196) -", "04:05:23 14:33:08"]
# ["34.162.236.44:8585", "HTTP", "ANM", "США ", "25% (1) -", "04:05:23 14:32:30"]
# ["45.143.98.2:3128", "HTTP", "HIA", "Турция !", "50% (1) +", "04:05:23 14:31:24"]
# ["123.200.26.214:8080", "HTTP !", "NOA", "Бангладеш !", "18% (34) -", "04:05:23 14:30:54"]
# ["34.151.245.107:3129", "HTTP", "ANM", "Бразилия !", "50% (1) -", "04:05:23 14:30:06"]
# ["138.117.86.153:999", "HTTP !", "NOA", "Колумбиа ", "42% (20) -", "04:05:23 14:29:17"]
# ["221.113.87.173:80", "HTTPS !", "NOA", "Япония !", "35% (9) -", "04:05:23 14:29:11"]


# ====================================================================================




# если перебрать циклом вывод вот такой
# IP
# ...........
# Дата
# проверки
# 65.109
# .12
# .131: 8080
# HTTPS !
# NOA
# Финляндия
# 100 % (5) +...........

# table_online_proxy2 = soup.findAll('tr',class_=['spy1x','spy1xx'])
# table_online_proxy2
# [<tr class="spy1x"><td colspan="1"><font class="spy2">IP адрес и порт</font></td><td colspan="1">
# <font class="spy2">Тип</font></td><td colspan="1"><font class="spy2">Анонимность</font></td><td colspan="1">
# <font class="spy2">Страна</font></td><td colspan="1"><font class="spy2">Аптайм</font></td><td colspan="1">
# <font class="spy2">Дата проверки</font></td></tr>, <tr class="spy1xx" onmouseout="this.style.background='#003333'"
# onmouseover="this.style.background='#002424'"><td colspan="1"><font class="spy14">65.109.12.131:8080</font></td>
# <td colspan="1"><font class="spy1"><acronym title="Squid">HTTP<font class="spy14">S</font>..........


# необходимо получить значения данных из строк
# первый столбец это значение IP адрес и порт
# adress_port = re.findall(r'/d', table_online_proxy)
# print(adress_port)
# не работает!


# print(response.status_code)
# print(response.headers)
# print(response.text)


# for row in table_online_proxy:
#         print(row.text)
# цикл выдает вот такой вид информации

# IP
# адрес
# ..............
# проверки
# 65.109
# .12
# .131: 8080
# HTTPS !
# S
# !
# NOA
# Финляндия
# 100 % (5) +
# (5)
# +
# 03: 05:23
# 22: 16:56................

# добавим новую переменную - row_11 - куда будем складывать наши результаты
# for row in table_online_proxy1:
#         print(row.text)
#         row_11.append(row.text)
# row_11 - вызвав эту переменную получим следующее
# ['IP адрес и порт', 'Тип', 'Анонимность', 'Страна', 'Аптайм', 'Дата проверки', '65.109.12.131:8080', 'HTTPS !',
# 'NOA', 'Финляндия ', '100% (5) +', '03:05:23 22:16:56', '179.255.219.182:8080', 'HTTP !', 'NOA', 'Бразилия ',
# '17% (26) -', '03:05:23 22:16:56', '20.212.147.91:3128', 'HTTP !', 'NOA', 'Сингапур ', '100% (2) +',
# '03:05:23 22:16:01', '124.158.183.82:8181', 'HTTP !', 'NOA', 'Индонезия ', '21% (18) -', '03:05:23 22:15:33',......

# row_11[6:-2] - отсеяли лишние данные!

# ['65.109.12.131:8080', 'HTTPS !', 'NOA', 'Финляндия ', '100% (5) +', '03:05:23 22:16:56', '179.255.219.182:8080',
# 'HTTP !', 'NOA', 'Бразилия ', '17% (26) -', '03:05:23 22:16:56', '20.212.147.91:3128', 'HTTP !', 'NOA', 'Сингапур ',
# '100% (2) +', '03:05:23 22:16:01', '124.158.183.82:8181', 'HTTP !', 'NOA', 'Индонезия ', '21% (18) -',
# '03:05:23 22:15:33', '36.255.85.218:32650', 'HTTP !', 'NOA', 'Индия ', '20% (30) -', '03:05:23 22:14:49',
# '64.227.188.77:8080', 'HTTPS !', 'NOA', 'Индия ', '100% (6) +', '03:05:23 22:13:59', '143.110.213.32:8080',
# 'HTTPS !', 'NOA', 'Канада ', '100% (7) +', '03:05:23 22:12:51', '124.122.2.110:8080', 'HTTP !', 'NOA', 'Таиланд ',
# '33% (7) -', '03:05:23 22:12:45', '134.209.189.42:80', 'HTTP', 'HIA', 'Англия ', '17% (498) -', '03:05:23 22:11:22',
# '38.56.70.97:999', 'HTTPS !', 'NOA', 'Доминиканская Республика ', '53% (201) -', '03:05:23 22:10:43',
# '103.69.108.78:8191', 'HTTPS', 'HIA', 'Филиппины ', '78% (191) -', '03:05:23 22:10:22', '103.143.197.19:8080',
# 'HTTP !', 'ANM', 'Индонезия !', '22% (28) -', '03:05:23 22:09:09', '181.209.80.134:999', 'HTTP !', 'NOA',
# 'Аргентина ', '17% (11) -', '03:05:23 22:08:39', '3.8.212.97:8888', 'HTTP', 'ANM', 'Англия ', '100% (3) +',
# '03:05:23 22:08:06', '89.117.32.209:80', 'HTTP', 'HIA', 'Бразилия ', '42% (45) +', '03:05:23 22:07:40',
# '23.132.185.101:53128', 'HTTP', 'HIA', 'США ', '35% (63) -', '03:05:23 22:07:08', '203.146.127.159:80',
# 'HTTP', 'HIA', 'Таиланд ', '34% (84) -', '03:05:23 22:06:57', '143.0.125.156:5566', 'HTTP !', 'NOA', 'Бразилия ',
# '21% (6) -', '03:05:23 22:06:43', '168.119.180.75:80', 'HTTP', 'ANM', 'Германия ', '100% (22) +',
# '03:05:23 22:04:26', '102.129.157.95:8080', 'HTTP', 'ANM', 'Камбоджа ', '100% (2) +', '03:05:23 22:04:11']

# row12 = list(zip(row_12[::6],row_12[1::6],row_12[2::6],row_12[3::6],row_12[4::6],row_12[5::6]))
# row12    # собрали данные в список, но это всё одной строкой
# [('65.109.12.131:8080', 'HTTPS !', 'NOA', 'Финляндия ', '100% (5) +', '03:05:23 22:16:56'),
# ('179.255.219.182:8080', 'HTTP !', 'NOA', 'Бразилия ', '17% (26) -', '03:05:23 22:16:56')........

# переберем еще раз этот список и получим результат в каждой строке
# for i in row12:
#     print(i)
# ('65.109.12.131:8080', 'HTTPS !', 'NOA', 'Финляндия ', '100% (5) +', '03:05:23 22:16:56')
# ('179.255.219.182:8080', 'HTTP !', 'NOA', 'Бразилия ', '17% (26) -', '03:05:23 22:16:56')
# ......... преобразовать в json


# необходимо вытащить данные из тегов <font class='spy1'>, <font class='spy1'>

# table_online_proxy2 = soup.findAll('tr', class_={'spy1x','spy1xx'})
# table_online_proxy2
# работает  даже лучше!!!

# for row in table_online_proxy2:
#     print(row.text)
# Вывод вот такой - Не подходит!!
# ...............
# 199.102.105.242:4145SOCKS5HIAСША 100% (11) -02:05:23 19:53:11
# 103.162.205.20:8181HTTPHIAИндонезия 13% (9) -02:05:23 19:52:54
# 207.180.235.47:24758SOCKS5HIAГермания 16% (10) -02:05:23 19:52:49
# US США (778)
# BR Бразилия (764)
# ID Индонезия (755)
# ............
# нужно убрать хвосты - последние данные после не содержащие прокси

# my_proxy_list[1]
# '167.114.19.195:8050HTTP !NOAКанада 16% (114) -02:05:23 20:05:49'
# my_proxy_list[1].split(' ')
# ['167.114.19.195:8050HTTP', '!NOAКанада', '16%', '(114)', '-02:05:23', '20:05:49']
# можно получить вот такие данные


# -----------------------------------------------------------------------------------
# Делаем программу в стиле ООП

# def get_proxy():
#     # ua = fake_user_agent()
#     headers = {
#         'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'}
#     url = 'https://spys.one/'
#     # response = requests.get(url, headers={'User-agent': ua})
#     response = requests.get(url, headers=headers)
#     print(response.status_code)
#     print(response.headers)
#     print(response.text)
